{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget mlflow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'usr (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wget\n",
    "import mlflow\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tqdm\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.utils import make_grid\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'MarceloRosendo'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '8424ba4e319e56a3077b43c49bec078fcdf9ca5a'\n",
    "mlflow.set_tracking_uri('.mlflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_gpus = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 2e-4\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "image_channels = 3\n",
    "z_dimension = 100\n",
    "num_epochs = 30\n",
    "discriminator_features = 64\n",
    "generator_features = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_root = \"datasets\"\n",
    "base_url = \"https://graal.ift.ulaval.ca/public/celeba/\"\n",
    "\n",
    "file_list = [\n",
    "    \"img_align_celeba.zip\",\n",
    "    \"list_attr_celeba.txt\",\n",
    "    \"identity_CelebA.txt\",\n",
    "    \"list_bbox_celeba.txt\",\n",
    "    \"list_landmarks_align_celeba.txt\",\n",
    "    \"list_eval_partition.txt\",\n",
    "]\n",
    "\n",
    "dataset_folder = f\"{data_root}/celeba\"\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "for file in file_list:\n",
    "    url = f\"{base_url}/{file}\"\n",
    "    if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
    "        wget.download(url, f\"{dataset_folder}/{file}\")\n",
    "\n",
    "with zipfile.ZipFile(f\"{dataset_folder}/img_align_celeba.zip\", \"r\") as ziphandler:\n",
    "    ziphandler.extractall(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                         std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_root, transform=transform)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Subset(dataset, np.arange(1, 20000))\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64],\n",
    "                                         padding=2,\n",
    "                                         normalize=True).cpu(),\n",
    "                        axes=(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 z_dimension: int,\n",
    "                 image_channels: int,\n",
    "                 generator_features: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Generator class.\n",
    "\n",
    "        Args:\n",
    "            z_dimension (int): The dimension of the input noise vector.\n",
    "            image_channels (int): The number of channels in the output image.\n",
    "            generator_features (int): The number of feature maps in the generator model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator_sequential_model = nn.Sequential(\n",
    "            self._conv_generator_block(in_channels=z_dimension,\n",
    "                                       out_channels=generator_features * 16,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=1,\n",
    "                                       padding=0),\n",
    "            self._conv_generator_block(in_channels=generator_features * 16,\n",
    "                                       out_channels=generator_features * 8,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "            self._conv_generator_block(in_channels=generator_features * 8,\n",
    "                                       out_channels=generator_features * 4,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "            self._conv_generator_block(in_channels=generator_features * 4,\n",
    "                                       out_channels=generator_features * 2,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=generator_features * 2,\n",
    "                out_channels=image_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv_generator_block(in_channels: int,\n",
    "                              out_channels: int,\n",
    "                              kernel_size: int,\n",
    "                              stride: int,\n",
    "                              padding: int) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Generate a convolutional generator block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            kernel_size (int): The size of the convolutional kernel.\n",
    "            stride (int): The stride of the convolution.\n",
    "            padding (int): The padding added to the input.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential module containing the convolutional generator block.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        return self.generator_sequential_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_channels: int,\n",
    "                 discriminator_features: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a Discriminator object.\n",
    "\n",
    "        Args:\n",
    "            image_channels (int): The number of channels in the input image.\n",
    "            discriminator_features (int): The number of features in the discriminator model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator_sequential_model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels,\n",
    "                      out_channels=discriminator_features,\n",
    "                      kernel_size=4,\n",
    "                      stride=2,\n",
    "                      padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._conv_discriminator_block(in_channels=discriminator_features,\n",
    "                                           out_channels=discriminator_features * 2,\n",
    "                                           kernel_size=4,\n",
    "                                           stride=2,\n",
    "                                           padding=1),\n",
    "            self._conv_discriminator_block(in_channels=discriminator_features * 2,\n",
    "                                           out_channels=discriminator_features * 4,\n",
    "                                           kernel_size=4,\n",
    "                                           stride=2,\n",
    "                                           padding=1),\n",
    "            self._conv_discriminator_block(in_channels=discriminator_features * 4,\n",
    "                                           out_channels=discriminator_features * 8,\n",
    "                                           kernel_size=4, stride=2,\n",
    "                                           padding=1),\n",
    "            nn.Conv2d(in_channels=discriminator_features * 8,\n",
    "                      out_channels=1,\n",
    "                      kernel_size=4,\n",
    "                      stride=2,\n",
    "                      padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv_discriminator_block(in_channels: int,\n",
    "                                  out_channels: int,\n",
    "                                  kernel_size: int,\n",
    "                                  stride: int,\n",
    "                                  padding: int) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Create a discriminator block consisting of a convolutional layer, batch normalization,\n",
    "        and a leaky ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            kernel_size (int): The size of the convolutional kernel.\n",
    "            stride (int): The stride value for the convolution.\n",
    "            padding (int): The padding value for the convolution.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: The discriminator block.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        return self.discriminator_sequential_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the weights of a neural network model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to initialize the weights for.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything.\n",
    "    \"\"\"\n",
    "    class_name = model.__class__.__name__\n",
    "    if 'Conv' in class_name:\n",
    "        nn.init.normal_(model.weight.data,\n",
    "                        mean=0.0,\n",
    "                        std=0.02)\n",
    "    elif 'BatchNorm' in class_name:\n",
    "        nn.init.normal_(model.weight.data,\n",
    "                        mean=1.0,\n",
    "                        std=0.02)\n",
    "        nn.init.constant_(model.bias.data,\n",
    "                          val=0)\n",
    "\n",
    "def show_tensor_images(image_tensor: Tensor,\n",
    "                       num_images: int = 32) -> None:\n",
    "    \"\"\"\n",
    "    Displays a grid of tensor images.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (Tensor): The input tensor containing the images.\n",
    "        num_images (int, optional): The number of images to display. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_detached = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_detached[:num_images], nrow=4)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def calculate_generator_loss(fake_image: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the generator loss.\n",
    "\n",
    "    Args:\n",
    "        fake_image (Tensor): The fake image generated by the generator.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The generator loss.\n",
    "\n",
    "    \"\"\"\n",
    "    output = discriminator(fake_image).reshape(-1)\n",
    "    return loss(output, torch.ones_like(output))\n",
    "\n",
    "\n",
    "def calculate_discriminator_loss(real_image: Tensor,\n",
    "                                 fake_image: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the discriminator loss for a batch of real and fake images.\n",
    "\n",
    "    Parameters:\n",
    "        real_image (torch.Tensor): A tensor representing the batch of real images.\n",
    "        fake_image (torch.Tensor): A tensor representing the batch of fake images.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the discriminator loss for the given batch of images.\n",
    "    \"\"\"\n",
    "    disc_real = discriminator(real_image).reshape(-1)\n",
    "    disc_fake = discriminator(fake_image.detach()).reshape(-1)\n",
    "    loss_disc_real = loss(disc_real, torch.ones_like(disc_real))\n",
    "    loss_disc_fake = loss(disc_fake, torch.zeros_like(disc_fake))\n",
    "    return (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "\n",
    "def show_intermediate_results() -> None:\n",
    "    \"\"\"\n",
    "    Display intermediate results during training.\n",
    "\n",
    "    This function prints the current epoch and batch information, as well as the discriminator and generator loss. It\n",
    "    also generates and displays a grid of fake images.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(f\" Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "            Loss D: {discriminator_loss:.4f}, loss G: {generator_loss:.4f}\")\n",
    "    with torch.no_grad():\n",
    "        fixed_fake_image = generator(fixed_noise)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fixed_fake_image[:32], normalize=True)\n",
    "        show_tensor_images(img_grid_fake)\n",
    "\n",
    "\n",
    "def plot_loss_convergence(mean_discriminator_loss: list,\n",
    "                          mean_generator_loss: list) -> None:\n",
    "    \"\"\"\n",
    "    Plot the convergence of the discriminator and generator loss.\n",
    "\n",
    "    Parameters:\n",
    "    - mean_discriminator_loss (list): A list of the mean discriminator loss values.\n",
    "    - mean_generator_loss (list): A list of the mean generator loss values.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.plot(mean_discriminator_loss, label='Discriminator')\n",
    "    plt.plot(mean_generator_loss, label='Generator')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z_dimension,\n",
    "                      image_channels,\n",
    "                      generator_features).to(device)\n",
    "discriminator = Discriminator(image_channels,\n",
    "                              discriminator_features).to(device)\n",
    "\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = optim.Adam(generator.parameters(),\n",
    "                                 lr=learning_rate,\n",
    "                                 betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(),\n",
    "                                     lr=learning_rate,\n",
    "                                     betas=(0.5, 0.999))\n",
    "loss = nn.BCELoss()\n",
    "fixed_noise = torch.randn(32, z_dimension, 1, 1).to(device)\n",
    "mean_discriminator_loss = []\n",
    "mean_generator_loss = []\n",
    "discriminator_loss = None\n",
    "generator_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "with mlflow.start_run(run_name=f'DCGAN-{uuid.uuid4()}'):\n",
    "    mlflow.log_params({\n",
    "        \"generator_optimizer\": generator_optimizer.__class__.__name__,\n",
    "        \"discriminator_optimizer\": discriminator_optimizer.__class__.__name__,\n",
    "        \"loss\": loss.__class__.__name__,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"z_dimension\": z_dimension,\n",
    "        \"image_channels\": image_channels,\n",
    "        \"discriminator_features\": discriminator_features,\n",
    "        \"generator_features\": generator_features,\n",
    "        \"number_of_gpus\": number_of_gpus,\n",
    "        \"image_size\": image_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"device\": device,\n",
    "        \"seed\":manualSeed\n",
    "    })\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        for batch_idx, (real_image, _) in enumerate(dataloader):\n",
    "            real_image = real_image.to(device)\n",
    "            noise = torch.randn((batch_size, z_dimension, 1, 1)).to(device)\n",
    "            fake_image = generator(noise)\n",
    "\n",
    "            discriminator_loss = calculate_discriminator_loss(real_image, fake_image)\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            generator_loss = calculate_generator_loss(fake_image)\n",
    "            generator_optimizer.zero_grad()\n",
    "            generator_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                show_intermediate_results()\n",
    "\n",
    "            mean_discriminator_loss.append(discriminator_loss.item())\n",
    "            mean_generator_loss.append(generator_loss.item())\n",
    "\n",
    "            mlflow.log_metrics(metrics={\"discriminator_loss\": discriminator_loss.item()},\n",
    "                               step=batch_idx)\n",
    "            mlflow.log_metrics(metrics={\"generator_loss\": generator_loss.item()},\n",
    "                               step=batch_idx)\n",
    "\n",
    "    mlflow.pytorch.log_model(pytorch_model=generator,\n",
    "                             artifact_path=\"generator_model\")\n",
    "    mlflow.pytorch.log_model(pytorch_model=discriminator,\n",
    "                             artifact_path=\"discriminator_model\")\n",
    "\n",
    "plot_loss_convergence(mean_discriminator_loss, mean_generator_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
