{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget mlflow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (101 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.56.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9686 sha256=375240cdfaabf4fdb1209e5bb2f1d04c44b76d15ee96fff9a73c5c54ad8a56b0\n",
      "  Stored in directory: /home/madmarx/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-77.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-77.0.3-py3-none-any.whl (1.3 MB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.0 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 setuptools-77.0.3 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from torchvision) (2.2.4)\n",
      "Requirement already satisfied: torch==2.6.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (77.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m486.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvutils\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Subset\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wget\n",
    "import mlflow\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tqdm\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.utils import make_grid\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m manualSeed = \u001b[32m999\u001b[39m\n\u001b[32m      2\u001b[39m random.seed(manualSeed)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtorch\u001b[49m.manual_seed(manualSeed)\n\u001b[32m      5\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mMLFLOW_TRACKING_USERNAME\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mMarceloRosendo\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mMLFLOW_TRACKING_PASSWORD\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33m8424ba4e319e56a3077b43c49bec078fcdf9ca5a\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "manualSeed = 999\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'MarceloRosendo'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '8424ba4e319e56a3077b43c49bec078fcdf9ca5a'\n",
    "mlflow.set_tracking_uri('.mlflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_gpus = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 2e-4\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "image_channels = 3\n",
    "z_dimension = 100\n",
    "num_epochs = 30\n",
    "discriminator_features = 64\n",
    "generator_features = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_root = \"datasets\"\n",
    "base_url = \"https://graal.ift.ulaval.ca/public/celeba/\"\n",
    "\n",
    "file_list = [\n",
    "    \"img_align_celeba.zip\",\n",
    "    \"list_attr_celeba.txt\",\n",
    "    \"identity_CelebA.txt\",\n",
    "    \"list_bbox_celeba.txt\",\n",
    "    \"list_landmarks_align_celeba.txt\",\n",
    "    \"list_eval_partition.txt\",\n",
    "]\n",
    "\n",
    "dataset_folder = f\"{data_root}/celeba\"\n",
    "os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "for file in file_list:\n",
    "    url = f\"{base_url}/{file}\"\n",
    "    if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
    "        wget.download(url, f\"{dataset_folder}/{file}\")\n",
    "\n",
    "with zipfile.ZipFile(f\"{dataset_folder}/img_align_celeba.zip\", \"r\") as ziphandler:\n",
    "    ziphandler.extractall(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                         std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_root, transform=transform)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Subset(dataset, np.arange(1, 20000))\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64],\n",
    "                                         padding=2,\n",
    "                                         normalize=True).cpu(),\n",
    "                        axes=(1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 z_dimension: int,\n",
    "                 image_channels: int,\n",
    "                 generator_features: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Generator class.\n",
    "\n",
    "        Args:\n",
    "            z_dimension (int): The dimension of the input noise vector.\n",
    "            image_channels (int): The number of channels in the output image.\n",
    "            generator_features (int): The number of feature maps in the generator model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.generator_sequential_model = nn.Sequential(\n",
    "            self._conv_generator_block(in_channels=z_dimension,\n",
    "                                       out_channels=generator_features * 16,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=1,\n",
    "                                       padding=0),\n",
    "            self._conv_generator_block(in_channels=generator_features * 16,\n",
    "                                       out_channels=generator_features * 8,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "            self._conv_generator_block(in_channels=generator_features * 8,\n",
    "                                       out_channels=generator_features * 4,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "            self._conv_generator_block(in_channels=generator_features * 4,\n",
    "                                       out_channels=generator_features * 2,\n",
    "                                       kernel_size=4,\n",
    "                                       stride=2,\n",
    "                                       padding=1),\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=generator_features * 2,\n",
    "                out_channels=image_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv_generator_block(in_channels: int,\n",
    "                              out_channels: int,\n",
    "                              kernel_size: int,\n",
    "                              stride: int,\n",
    "                              padding: int) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Generate a convolutional generator block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            kernel_size (int): The size of the convolutional kernel.\n",
    "            stride (int): The stride of the convolution.\n",
    "            padding (int): The padding added to the input.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential module containing the convolutional generator block.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        return self.generator_sequential_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_channels: int,\n",
    "                 discriminator_features: int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes a Discriminator object.\n",
    "\n",
    "        Args:\n",
    "            image_channels (int): The number of channels in the input image.\n",
    "            discriminator_features (int): The number of features in the discriminator model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.discriminator_sequential_model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels,\n",
    "                      out_channels=discriminator_features,\n",
    "                      kernel_size=4,\n",
    "                      stride=2,\n",
    "                      padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._conv_discriminator_block(in_channels=discriminator_features,\n",
    "                                           out_channels=discriminator_features * 2,\n",
    "                                           kernel_size=4,\n",
    "                                           stride=2,\n",
    "                                           padding=1),\n",
    "            self._conv_discriminator_block(in_channels=discriminator_features * 2,\n",
    "                                           out_channels=discriminator_features * 4,\n",
    "                                           kernel_size=4,\n",
    "                                           stride=2,\n",
    "                                           padding=1),\n",
    "            self._conv_discriminator_block(in_channels=discriminator_features * 4,\n",
    "                                           out_channels=discriminator_features * 8,\n",
    "                                           kernel_size=4, stride=2,\n",
    "                                           padding=1),\n",
    "            nn.Conv2d(in_channels=discriminator_features * 8,\n",
    "                      out_channels=1,\n",
    "                      kernel_size=4,\n",
    "                      stride=2,\n",
    "                      padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _conv_discriminator_block(in_channels: int,\n",
    "                                  out_channels: int,\n",
    "                                  kernel_size: int,\n",
    "                                  stride: int,\n",
    "                                  padding: int) -> nn.Sequential:\n",
    "        \"\"\"\n",
    "        Create a discriminator block consisting of a convolutional layer, batch normalization,\n",
    "        and a leaky ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels.\n",
    "            out_channels (int): The number of output channels.\n",
    "            kernel_size (int): The size of the convolutional kernel.\n",
    "            stride (int): The stride value for the convolution.\n",
    "            padding (int): The padding value for the convolution.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: The discriminator block.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "        return self.discriminator_sequential_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Initializes the weights of a neural network model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to initialize the weights for.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return anything.\n",
    "    \"\"\"\n",
    "    class_name = model.__class__.__name__\n",
    "    if 'Conv' in class_name:\n",
    "        nn.init.normal_(model.weight.data,\n",
    "                        mean=0.0,\n",
    "                        std=0.02)\n",
    "    elif 'BatchNorm' in class_name:\n",
    "        nn.init.normal_(model.weight.data,\n",
    "                        mean=1.0,\n",
    "                        std=0.02)\n",
    "        nn.init.constant_(model.bias.data,\n",
    "                          val=0)\n",
    "\n",
    "def show_tensor_images(image_tensor: Tensor,\n",
    "                       num_images: int = 32) -> None:\n",
    "    \"\"\"\n",
    "    Displays a grid of tensor images.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (Tensor): The input tensor containing the images.\n",
    "        num_images (int, optional): The number of images to display. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_detached = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_detached[:num_images], nrow=4)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def calculate_generator_loss(fake_image: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the generator loss.\n",
    "\n",
    "    Args:\n",
    "        fake_image (Tensor): The fake image generated by the generator.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The generator loss.\n",
    "\n",
    "    \"\"\"\n",
    "    output = discriminator(fake_image).reshape(-1)\n",
    "    return loss(output, torch.ones_like(output))\n",
    "\n",
    "\n",
    "def calculate_discriminator_loss(real_image: Tensor,\n",
    "                                 fake_image: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the discriminator loss for a batch of real and fake images.\n",
    "\n",
    "    Parameters:\n",
    "        real_image (torch.Tensor): A tensor representing the batch of real images.\n",
    "        fake_image (torch.Tensor): A tensor representing the batch of fake images.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor representing the discriminator loss for the given batch of images.\n",
    "    \"\"\"\n",
    "    disc_real = discriminator(real_image).reshape(-1)\n",
    "    disc_fake = discriminator(fake_image.detach()).reshape(-1)\n",
    "    loss_disc_real = loss(disc_real, torch.ones_like(disc_real))\n",
    "    loss_disc_fake = loss(disc_fake, torch.zeros_like(disc_fake))\n",
    "    return (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "\n",
    "def show_intermediate_results() -> None:\n",
    "    \"\"\"\n",
    "    Display intermediate results during training.\n",
    "\n",
    "    This function prints the current epoch and batch information, as well as the discriminator and generator loss. It\n",
    "    also generates and displays a grid of fake images.\n",
    "\n",
    "    Parameters:\n",
    "    - None\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(f\" Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "            Loss D: {discriminator_loss:.4f}, loss G: {generator_loss:.4f}\")\n",
    "    with torch.no_grad():\n",
    "        fixed_fake_image = generator(fixed_noise)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fixed_fake_image[:32], normalize=True)\n",
    "        show_tensor_images(img_grid_fake)\n",
    "\n",
    "\n",
    "def plot_loss_convergence(mean_discriminator_loss: list,\n",
    "                          mean_generator_loss: list) -> None:\n",
    "    \"\"\"\n",
    "    Plot the convergence of the discriminator and generator loss.\n",
    "\n",
    "    Parameters:\n",
    "    - mean_discriminator_loss (list): A list of the mean discriminator loss values.\n",
    "    - mean_generator_loss (list): A list of the mean generator loss values.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    plt.plot(mean_discriminator_loss, label='Discriminator')\n",
    "    plt.plot(mean_generator_loss, label='Generator')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z_dimension,\n",
    "                      image_channels,\n",
    "                      generator_features).to(device)\n",
    "discriminator = Discriminator(image_channels,\n",
    "                              discriminator_features).to(device)\n",
    "\n",
    "initialize_weights(generator)\n",
    "initialize_weights(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = optim.Adam(generator.parameters(),\n",
    "                                 lr=learning_rate,\n",
    "                                 betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(),\n",
    "                                     lr=learning_rate,\n",
    "                                     betas=(0.5, 0.999))\n",
    "loss = nn.BCELoss()\n",
    "fixed_noise = torch.randn(32, z_dimension, 1, 1).to(device)\n",
    "mean_discriminator_loss = []\n",
    "mean_generator_loss = []\n",
    "discriminator_loss = None\n",
    "generator_loss = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "with mlflow.start_run(run_name=f'DCGAN-{uuid.uuid4()}'):\n",
    "    mlflow.log_params({\n",
    "        \"generator_optimizer\": generator_optimizer.__class__.__name__,\n",
    "        \"discriminator_optimizer\": discriminator_optimizer.__class__.__name__,\n",
    "        \"loss\": loss.__class__.__name__,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"z_dimension\": z_dimension,\n",
    "        \"image_channels\": image_channels,\n",
    "        \"discriminator_features\": discriminator_features,\n",
    "        \"generator_features\": generator_features,\n",
    "        \"number_of_gpus\": number_of_gpus,\n",
    "        \"image_size\": image_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"device\": device,\n",
    "        \"seed\":manualSeed\n",
    "    })\n",
    "\n",
    "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "        for batch_idx, (real_image, _) in enumerate(dataloader):\n",
    "            real_image = real_image.to(device)\n",
    "            noise = torch.randn((batch_size, z_dimension, 1, 1)).to(device)\n",
    "            fake_image = generator(noise)\n",
    "\n",
    "            discriminator_loss = calculate_discriminator_loss(real_image, fake_image)\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            generator_loss = calculate_generator_loss(fake_image)\n",
    "            generator_optimizer.zero_grad()\n",
    "            generator_loss.backward()\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                show_intermediate_results()\n",
    "\n",
    "            mean_discriminator_loss.append(discriminator_loss.item())\n",
    "            mean_generator_loss.append(generator_loss.item())\n",
    "\n",
    "            mlflow.log_metrics(metrics={\"discriminator_loss\": discriminator_loss.item()},\n",
    "                               step=batch_idx)\n",
    "            mlflow.log_metrics(metrics={\"generator_loss\": generator_loss.item()},\n",
    "                               step=batch_idx)\n",
    "\n",
    "    mlflow.pytorch.log_model(pytorch_model=generator,\n",
    "                             artifact_path=\"generator_model\")\n",
    "    mlflow.pytorch.log_model(pytorch_model=discriminator,\n",
    "                             artifact_path=\"discriminator_model\")\n",
    "\n",
    "plot_loss_convergence(mean_discriminator_loss, mean_generator_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
